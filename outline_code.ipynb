{
 "cells": [
  {
   "cell_type": "code",
   "id": "c0fedf92-b08d-4833-ac2c-966edf4384cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:03:25.151154Z",
     "start_time": "2025-06-27T14:03:14.874177Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/vehicles.csv\")\n",
    "print(df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426880, 26)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ## Data Cleaning:\n",
    " ### Removed unnecessary columns (url, VIN, image_url, etc.)\n",
    " ### filtered edge cases - kept prices $1K-80K$, car age 0-30 years, odometer <1M miles"
   ],
   "id": "8f4363e093ed3230"
  },
  {
   "cell_type": "code",
   "id": "7bc158af-f045-4dbb-aa2c-dfcf62763928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:03:25.636196Z",
     "start_time": "2025-06-27T14:03:25.163172Z"
    }
   },
   "source": [
    "# remove column\n",
    "df.drop(columns=['url', 'region_url', 'VIN', 'image_url', 'description', \n",
    "                 'county', 'size', 'lat', 'long', 'title_status'], inplace=True, errors='ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "9f5f2cca-6bdc-4f0f-960f-16c6d8f26856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:03:26.751835Z",
     "start_time": "2025-06-27T14:03:25.642590Z"
    }
   },
   "source": [
    "# postdate convert to datetime\n",
    "df['posting_date'] = pd.to_datetime(df['posting_date'], errors='coerce', utc=True)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0167c080-1f42-4bd6-ab9b-84db81b7ee41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:03:26.768962Z",
     "start_time": "2025-06-27T14:03:26.757960Z"
    }
   },
   "source": [
    "# adding car age column\n",
    "df['car_age'] = df['posting_date'].dt.year - df['year']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:03:26.912967Z",
     "start_time": "2025-06-27T14:03:26.776106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run this to check remaining missing values\n",
    "print(\"Remaining missing values by column:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False))\n",
    "print(f\"\\nTotal missing: {df.isnull().sum().sum()}\")"
   ],
   "id": "debaa84e9b537860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values by column:\n",
      "cylinders       177678\n",
      "condition       174104\n",
      "drive           130567\n",
      "paint_color     130203\n",
      "type             92858\n",
      "manufacturer     17646\n",
      "model             5277\n",
      "odometer          4400\n",
      "fuel              3013\n",
      "transmission      2556\n",
      "year              1205\n",
      "car_age           1205\n",
      "posting_date        68\n",
      "id                   0\n",
      "region               0\n",
      "price                0\n",
      "state                0\n",
      "dtype: int64\n",
      "\n",
      "Total missing: 740780\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:06:02.240593Z",
     "start_time": "2025-06-27T14:06:02.217282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_region = df['region'].nunique()\n",
    "num_state  = df['state'].nunique()\n",
    "print(f\"region unique values: {num_region}\")\n",
    "print(f\"state unique values:  {num_state}\")"
   ],
   "id": "b64445788fdf0749",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region unique values: 404\n",
      "state unique values:  51\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "d5915202-03e0-416c-9c44-47b4a10454e6",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(np.log1p(df['price']), bins=50)\n",
    "plt.title('Log-Scaled Price Distribution')\n",
    "plt.xlabel('log(Price + 1)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df['car_age'], bins=30)\n",
    "plt.title('Car Age Distribution')\n",
    "plt.xlabel('Car Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e7c558f-40c9-4311-8bfd-a450530a57ed",
   "metadata": {},
   "source": [
    "# remove edge value\n",
    "df = df[(df['price'] >= 1000) & (df['price'] <= 80000)]\n",
    "df = df[df['car_age'].between(0, 30, inclusive='both')]\n",
    "df = df[df['odometer'] <= 1000000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Feature Engineering:\n",
    "### Created car_age column, filled missing values with 'unknown'\n",
    "### applied one-hot encoding to categorical features"
   ],
   "id": "c39ea3afa0b670a9"
  },
  {
   "cell_type": "code",
   "id": "19cd1e1a-8e50-4f10-83be-ea12b21f6601",
   "metadata": {},
   "source": [
    "# remove necessary empty value row\n",
    "df.dropna(subset=['price', 'year', 'manufacturer', 'model', 'odometer'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b685c97-f316-489e-8a81-9a4b6d4904ae",
   "metadata": {},
   "source": [
    "# fill empty value with 'unknown'\n",
    "fill_unknown_cols = ['condition', 'cylinders', 'fuel', 'drive', 'transmission', 'paint_color', 'type']\n",
    "for col in fill_unknown_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('unknown')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0306ead-4b0a-4265-abd0-e66e140b16f2",
   "metadata": {},
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f5ba642-2398-4dad-949c-7838e387c5d3",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Price\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['price'], bins=50, kde=True)\n",
    "plt.title('Price Distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 80000)\n",
    "plt.show()\n",
    "\n",
    "# car age\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['car_age'], bins=30, kde=True)\n",
    "plt.title('Car Age Distribution')\n",
    "plt.xlabel('Car Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# price and car age\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='car_age', y='price', data=df, alpha=0.3)\n",
    "plt.title('Price vs. Car Age')\n",
    "plt.xlabel('Car Age')\n",
    "plt.ylabel('Price')\n",
    "plt.ylim(0, 80000)\n",
    "plt.show()\n",
    "\n",
    "# price Odometer\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='odometer', y='price', data=df, alpha=0.3)\n",
    "plt.title('Price vs. Odometer')\n",
    "plt.xlabel('Odometer (miles)')\n",
    "plt.ylabel('Price')\n",
    "plt.ylim(0, 80000)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Category features EDA",
   "id": "d69407ae-156a-4101-bce4-fb1a29102ecb"
  },
  {
   "cell_type": "code",
   "id": "3418fec3-0d88-45de-b0da-70b7062ac6ac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Function: plot average price by category\n",
    "def plot_avg_price_by_category(col_name, top_n=10):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate average price and sort\n",
    "    top_categories = df.groupby(col_name)['price'].mean().sort_values(ascending=False).head(top_n).index\n",
    "    filtered_df = df[df[col_name].isin(top_categories)]\n",
    "\n",
    "    sns.barplot(x=col_name, y='price', data=filtered_df, order=top_categories)\n",
    "    plt.title(f'Average Price by {col_name.capitalize()}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Average Price')\n",
    "    plt.xlabel(col_name.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# top 10 manufacturers by avg price\n",
    "plot_avg_price_by_category('manufacturer')\n",
    "\n",
    "# top 10 models by avg price\n",
    "plot_avg_price_by_category('model')\n",
    "\n",
    "# top fuel types by avg price\n",
    "plot_avg_price_by_category('fuel')\n",
    "\n",
    "# top transmission types by avg price\n",
    "plot_avg_price_by_category('transmission')\n",
    "\n",
    "# top drive types by avg price\n",
    "plot_avg_price_by_category('drive')\n",
    "\n",
    "# top vehicle types by avg price\n",
    "plot_avg_price_by_category('type')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### apply one-hot encoding to categorical features",
   "id": "899a2fdf3a4936da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "num_cols = ['year', 'car_age', 'odometer']\n",
    "X_num = df[num_cols].reset_index(drop=True)\n",
    "\n",
    "# one hot\n",
    "low_cats = ['manufacturer', 'condition', 'cylinders', 'fuel',\n",
    "          'transmission', 'drive', 'type', 'paint_color']\n",
    "low_cats = [col for col in low_cats if col in df.columns]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_cat = encoder.fit_transform(df[low_cats])\n",
    "cat_names = encoder.get_feature_names_out(low_cats)\n",
    "X_cat = pd.DataFrame(X_cat, columns=cat_names)\n",
    "\n",
    "# label encode\n",
    "high_cats = ['model', 'state', 'region']\n",
    "high_cats = [col for col in high_cats if col in df.columns]\n",
    "\n",
    "X_high_list = []\n",
    "for col in high_cats:\n",
    "   le = LabelEncoder()\n",
    "   encoded = le.fit_transform(df[col].astype(str))\n",
    "   X_high_list.append(pd.DataFrame({f'{col}_enc': encoded}))\n",
    "\n",
    "X_high = pd.concat(X_high_list, axis=1) if X_high_list else pd.DataFrame()\n",
    "\n",
    "X_baseline = pd.concat([X_num, X_cat, X_high], axis=1)\n",
    "\n",
    "print(f\"Shape: {X_baseline.shape}\")\n",
    "print(f\"Numeric: {len(num_cols)}, One-hot: {X_cat.shape[1]}, Label: {X_high.shape[1]}\")"
   ],
   "id": "bb4eebc3-66c9-4e45-9c07-2f27c84de730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## XGBoost Baseline:\n",
    "### Feature Importance Analysis\n",
    "### Use XGBoost feature importance to identify top 5 impactful features\n",
    "### 87% accuracy (R²: 0.8778)"
   ],
   "id": "3217cfb845cf2e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    device = 'cuda'\n",
    "    print(\"Using GPU\")\n",
    "except:\n",
    "    device = 'cpu'\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "X = X_baseline\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train\n",
    "model = XGBRegressor(\n",
    "   n_estimators=200,\n",
    "   learning_rate=0.1,\n",
    "   max_depth=6,\n",
    "   tree_method='hist',\n",
    "   device='cuda',\n",
    "   random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# feature importance\n",
    "feat_imp = pd.DataFrame({\n",
    "   'feature': X.columns,\n",
    "   'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(f\"\\nTop 10 important features:\")\n",
    "for i, row in feat_imp.head(10).iterrows():\n",
    "   print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# feature importance\n",
    "axes[0].barh(feat_imp['feature'], feat_imp['importance'])\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].set_title(\"Top 20 Features (XGBoost)\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# predicted vs actual\n",
    "axes[1].scatter(y_test, y_pred, alpha=0.3)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[1].set_xlabel(\"Actual Price\")\n",
    "axes[1].set_ylabel(\"Predicted Price\")\n",
    "axes[1].set_title(\"Predicted vs Actual\")\n",
    "\n",
    "# residuals\n",
    "residuals = y_test - y_pred\n",
    "axes[2].scatter(y_test, residuals, alpha=0.3)\n",
    "axes[2].axhline(0, color='red', linestyle='--')\n",
    "axes[2].set_xlabel(\"Actual Price\")\n",
    "axes[2].set_ylabel(\"Residuals\")\n",
    "axes[2].set_title(\"Residual Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c219a020010046c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing for Deep Learning:\n",
    "### Apply StandardScaler to numeric features and RareCategoryCombiner for categorical features"
   ],
   "id": "c57e99596c91cb94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_features = ['car_age', 'odometer']\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(df[numeric_features])\n",
    "X_numeric_scaled = pd.DataFrame(X_numeric_scaled, columns=numeric_features)\n",
    "\n",
    "X_categorical = X_baseline.drop(columns=numeric_features).reset_index(drop=True)\n",
    "\n",
    "# combine\n",
    "X_full_scaled = pd.concat([X_numeric_scaled.reset_index(drop=True), X_categorical], axis=1)\n",
    "\n",
    "print(X_full_scaled.shape)\n",
    "X_full_scaled.head()"
   ],
   "id": "93b46a618fabace6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MLP Model (DL)\n",
    "### 92% accuracy (R²: 0.9257)"
   ],
   "id": "4dd77ab4c0f624e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "id": "3205f22e3a93b348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_clean = df.dropna(subset=['model', 'region']).copy()\n",
    "\n",
    "# num features\n",
    "num_cols = ['year', 'car_age', 'odometer']\n",
    "X_num = df_clean[num_cols].copy()\n",
    "\n",
    "# categoricals feature\n",
    "high_card_cols = ['model', 'region']\n",
    "encoders = {}\n",
    "X_high_encoded = {}\n",
    "\n",
    "for col in high_card_cols:\n",
    "   le = LabelEncoder()\n",
    "   X_high_encoded[col] = le.fit_transform(df_clean[col].astype(str))\n",
    "   encoders[col] = le\n",
    "   print(f\"{col}: {len(le.classes_)} unique categories\")\n",
    "\n",
    "# one hot\n",
    "low_card_cols = ['manufacturer', 'fuel', 'transmission', 'drive', 'type']\n",
    "low_card_cols = [col for col in low_card_cols if col in df_clean.columns]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_low_encoded = ohe.fit_transform(df_clean[low_card_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "y = df_clean['price'].values"
   ],
   "id": "3998231f72246be8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train test split\n",
    "\n",
    "indices = range(len(y))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split all features\n",
    "X_num_train, X_num_test = X_num_scaled[train_idx], X_num_scaled[test_idx]\n",
    "X_low_train, X_low_test = X_low_encoded[train_idx], X_low_encoded[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_high_train, X_high_test = {}, {}\n",
    "for col in high_card_cols:\n",
    "   X_high_train[col] = X_high_encoded[col][train_idx]\n",
    "   X_high_test[col] = X_high_encoded[col][test_idx]"
   ],
   "id": "1383ab26294768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_input = Input(shape=(len(num_cols),), name='numeric')\n",
    "num_dense = Dense(64, activation='relu')(num_input)\n",
    "\n",
    "embedding_layers = []\n",
    "for col in high_card_cols:\n",
    "   vocab_size = len(encoders[col].classes_)\n",
    "   embed_dim = min(50, vocab_size // 2)\n",
    "\n",
    "   input_layer = Input(shape=(1,), name=f'{col}_input')\n",
    "   embed_layer = Embedding(vocab_size, embed_dim, name=f'{col}_embed')(input_layer)\n",
    "   embed_flat = Flatten()(embed_layer)\n",
    "   embedding_layers.append((input_layer, embed_flat))\n",
    "\n",
    "low_input = Input(shape=(X_low_encoded.shape[1],), name='categorical')\n",
    "low_dense = Dense(64, activation='relu')(low_input)\n",
    "\n",
    "all_inputs = [num_input, low_input] + [layer[0] for layer in embedding_layers]\n",
    "all_features = [num_dense, low_dense] + [layer[1] for layer in embedding_layers]\n",
    "\n",
    "concat = Concatenate()(all_features)\n",
    "\n",
    "# Deep layers\n",
    "x = Dense(256, activation='relu')(concat)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(1, name='price')(x)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "   loss='mse',\n",
    "   metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"Model summary:\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ],
   "id": "22f0d90899980624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train\n",
    "train_data = [X_num_train, X_low_train] + [X_high_train[col].reshape(-1, 1) for col in high_card_cols]\n",
    "test_data = [X_num_test, X_low_test] + [X_high_test[col].reshape(-1, 1) for col in high_card_cols]\n",
    "\n",
    "history = model.fit(\n",
    "   train_data, y_train,\n",
    "   validation_split=0.2,\n",
    "   epochs=20,\n",
    "   batch_size=256,\n",
    "   verbose=1\n",
    ")\n",
    "\n",
    "y_pred = model.predict(test_data).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMulti-Input Neural Network Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ],
   "id": "965ac37e46eb7969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ec70e58c20d5b21f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding Model\n",
    "### Implemented embedding layer for high-cardinality 'model' feature and used deep MLP to achieve 92% accuracy (R²: 0.9251)"
   ],
   "id": "41cf4520e183f79b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "df_clean = df[df['model'].notna()].copy()\n",
    "y = df_clean['price'].values\n",
    "\n",
    "embed_cols = ['model', 'manufacturer']\n",
    "encoders = {}\n",
    "X_embed = {}\n",
    "\n",
    "for col in embed_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_embed[col] = le.fit_transform(df_clean[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "num_cols = ['year', 'car_age', 'odometer']\n",
    "cat_cols = ['fuel', 'transmission', 'drive', 'type']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "X_other = preprocessor.fit_transform(df_clean)\n",
    "\n",
    "# train test split\n",
    "indices = range(len(y))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "X_other_train, X_other_test = X_other[train_idx], X_other[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_embed_train, X_embed_test = {}, {}\n",
    "for col in embed_cols:\n",
    "    X_embed_train[col] = X_embed[col][train_idx]\n",
    "    X_embed_test[col] = X_embed[col][test_idx]\n",
    "\n",
    "inputs = []\n",
    "embeddings = []\n",
    "\n",
    "for col in embed_cols:\n",
    "    vocab_size = len(encoders[col].classes_)\n",
    "    embed_dim = min(16, vocab_size // 4)\n",
    "\n",
    "    input_layer = Input(shape=(1,), name=f'{col}_input')\n",
    "    embed_layer = Embedding(vocab_size, embed_dim, name=f'{col}_embed')(input_layer)\n",
    "    embed_flat = Flatten()(embed_layer)\n",
    "    inputs.append(input_layer)\n",
    "    embeddings.append(embed_flat)\n",
    "\n",
    "input_other = Input(shape=(X_other_train.shape[1],), name='other_features')\n",
    "inputs.append(input_other)\n",
    "\n",
    "if embeddings:\n",
    "    x = Concatenate()(embeddings + [input_other])\n",
    "else:\n",
    "    x = input_other\n",
    "\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x1 = Dense(128, activation='relu')(x)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "\n",
    "x2 = Dense(128, activation='relu')(x1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "\n",
    "x_skip = Dense(128)(x)\n",
    "x = Add()([x2, x_skip])\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(1, name='price')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "train_data = [X_embed_train[col].reshape(-1, 1) for col in embed_cols] + [X_other_train]\n",
    "test_data = [X_embed_test[col].reshape(-1, 1) for col in embed_cols] + [X_other_test]\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = model.predict(test_data).flatten()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAdvanced MLP with Multiple Embeddings Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ],
   "id": "fa26bd2bf09e52b5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
